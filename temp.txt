import os
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import TextLoader, UnstructuredWordDocumentLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

try:
    from langchain.chains.combine_documents import create_stuff_documents_chain
except ImportError:
    # LangChain >= 0.2 moved the factory into the base module
    from langchain.chains.combine_documents.base import create_stuff_documents_chain

try:
    from langchain.chains import create_retrieval_chain
except ImportError:
    # Newer LangChain releases expose the helper under chains.retrieval
    from langchain.chains.retrieval import create_retrieval_chain

def build_rag_chain():
    docs_dir = "backend/resume_data"
    all_docs = []

    # ✅ Load all .txt and .docx files
    for root, _, files in os.walk(docs_dir):
        for f in files:
            full_path = os.path.join(root, f)
            if f.endswith(".txt"):
                loader = TextLoader(full_path)
            elif f.endswith(".docx"):
                loader = UnstructuredWordDocumentLoader(full_path)
            else:
                continue
            all_docs.extend(loader.load())

    # ✅ Split into chunks
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    split_docs = splitter.split_documents(all_docs)

    # ✅ Embed and store
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(split_docs, embeddings)
    retriever = db.as_retriever(search_kwargs={"k": 3})

    # ✅ New LangChain 1.x chain creation
    llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)
    doc_chain = create_stuff_documents_chain(llm)
    rag_chain = create_retrieval_chain(retriever, doc_chain)

    return rag_chain
